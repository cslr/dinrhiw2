#summary documents unix command-line tools

= dstool =

Dstool is a helper program for nntool which will be probably merged with nntool at some point. It reads comma-separated CVS-formatted ASCII data and stores it into a format nntool can understand. 

*DATAFILE* <=> *DSTOOL* <=> *NNTOOL*

Command-line arguments of dstool are:
{{{
Usage: datatool <command> <datafile> [asciifile | datafile]
A tool for manipulating whiteice::dataset files.

 -list                      lists clusters, number of datapoints, preprocessings.
                            (default action)
 -print[:<c1>[:<b>[:<e>]]]  prints contents of cluster c1 (indexes [<b>,<e>])
 -create                    creates new empty dataset (<dataset> file doesn't exist)
 -create:<dim>[:name]       creates new empty <dim> dimensional dataset cluster
 -import:<c1>               imports data from comma separated CSV ascii file to cluster c1
 -export:<c1>               exports data from cluster c1 to comma separated CSV ascii file
 -add:<c1>:<c2>             adds data from another datafile cluster c2 to c1
 -move:<c1>:<c2>            moves data (internally) from cluster c2 to c1
 -copy:<c1>:<c2>            copies data (internally) from cluster c2 to c1
 -clear:<c1>                clears dataset cluster c1 (but doesn't remove cluster)
 -remove:<c1>               removes dataset cluster c1
 -padd:<c1>:<name>+         adds preprocessing(s) to cluster
 -premove:<c1>:<name>+      removes preprocesing(s) from cluster
                            preprocess names: meanvar, outlier, pca, ica
                            note: ica implementation is unstable and may not work
 -data:N                    jointly resamples all cluster sizes down to N datapoints

This program is distributed under LGPL license <dinrhiw2.googlecode.com>.
}}}

  # The *list* command lists dataset information: the number of datapoints, clusters and their preprocessing methods. It exists just for informational and testing purposes.
  # Similarly, the *print* command prints given clusters datapoints for debugging purposes.
  # The *create* command creates a new cluster into dataset containing D dimensions. The importing of data into dataset always starts with this command.
  # *import* and *export* commands are used to bring and save data to/from dataset and are most important commands that one needs to use when wanting to process datafile with nntool.
  # *add*, *move*, *copy*, *clear* and *remove* commands are seldom used and can be used to move datapoints between clusters
  # the *padd* command another essential command that can be used to preprocess data before it is fed to neural network. The _meanvar_ is maybe the most essential command here as it removed mean and normalizes variance of (input) data to unity making it nice to work with nntool's neural network implementation.
  # *premove* command can be used to remove preprocessings from data.
  # *data* command JOINTLY downsamples number of datapoints from all clusters down to N datapoints. This requires that number of datapoints in each cluster is  the same in each cluster (the usual case). 


= nntool = 

NNtool is the primary program in dinrhiw2. It is used to create, train and use neural networks. It reads dataset files created by dstool and stores results back to dataset files or text-formated neural network configuration files. It doesn't show results anyway but only prints some machine learning statistics and estimated time it takes to complete the (time consuming) commands. Many of its algorithms take advantage of multicore CPUs through multithreading and use  optimized BLAS-libraries to carry out computations.

Arguments of the program are: 

{{{
Usage: nntool [options] [data] [arch] <nnfile> [lmethod]
Create, train and use neural network(s).

-v             shows ETA and other details
--help         shows this help
--version      displays version and exits
--no-init      do not use heuristics when initializing nn weights
--overfit      do not use early stopping (bfgs,lbfgs)
--adaptive     use adaptive step length in bayesian hamiltonian monte carlo (bayes)
--negfb        use negative feedback between neurons (grad,parallelgrad,bfgs,lbfgs)
--load         use previously computed network weights as the starting point (grad,bfgs,lbfgs,bayes)
--time TIME    sets time limit for multistart optimization and bayesian inference
--samples N    samples N samples or defines max iterations (eg. 2500) to be used in optimization/sampling
--threads N    uses N parallel threads when looking for solution
--data N       takes randomly N samples of data for the learning process (N/2 used in training)
[data]         a source file for inputs or i/o examples (binary file)
               (whiteice data file format created by dstool)
[arch]         the architecture of a new nn. Eg. 3-10-9 or ?-10-?
<nnfile>       input/output neural networks weights file
[lmethod]      method: use, random, grad, parallelgrad, bayes, lbfgs, parallelbfgs, parallellbfgs
               parallel methods use random location multistart/restart parallel search
               until timeout or the number of samples has been reached
               additionally: minimize method finds input that minimizes the neural network output
               gradient descent algorithms use negative feedback heuristic

               Ctrl-C shutdowns the program gracefully.

Report bugs to <dinrhiw2.googlecode.com>.
}}}

  # The *v* option tells nntool to be more verbose printing out many informational messages about what is happening.
  # The *help* option prints out long list of commands.
  # *No-init* option tells nntool not to use default weight initialization heuristics (still used by parallelgrad,bfgs,lbfgs) but start from a random weight initialization.
  # *Overfit* option tells nntool to continue optimization desprite the cross-validation (testing dataset) tells it should do early stopping (the default).
  # *Adaptive* option is used my bayes/hmc sampler to use adaptive step length heuristics which should improve sampler's convergence to the target distribution (posterior distribution of neural network weights).
  # The *negfb* option activates "negative feedback between neurons" heuristic during training which can be very useful to force different neurons to behave independently (otherwise different neuronal units can be correlated) and use neural network's capacity efficiently.
  # *load* option tells to load network from the disk and start training from it instead of starting from the scratch. This is useful when you want to use different training algorithms in sequence.
  # 