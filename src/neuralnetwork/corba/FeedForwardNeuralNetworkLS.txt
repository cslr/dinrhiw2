
Documentation/Implementation Notes


FeedforwardNeuralNetworkLS CORBA component implements
traditional feedforward neural network learning system.
The approach discribed below borrows ideas from bayesian
inference and tries to do learning similar to
bayesian inference with a heuristical methods so that
computational burden of baeysian methods wouldn't be problem.


Implementation always transforms data to be zero mean and
non-correlating if total number of data points for calculating
PCA at some point of the learning process is large enough for
estimates of first and second order statistics to be good enough.

This PCA maybe also updated later. Feedforward neural network
tries to remember as many as possible older training examples
as long as amount of free memory doesn't exceed the physical
memory available.
Also outputs will be whitened similarly with PCA.

The learning system uses particle swarm optimization (PSO) with
restarting for finding well performing neural network. Number of
restarts is the first parameter of meta-level Genetic Algorithm 
optimzer.

The PSO will optimize fNN with fixed architecture. Besides this
learning approach uses basic Genetic Algorithm (GA) for competing
against different PSO clusters with different NN architectures.
During each GA optimization iteration new smaller and bigger NN
candidates (more layers) are introduced. Joining two unequal size
NN architectures (different number of layers) happens by zero
extending the smaller NN layer architecture.

Because fNN's must solve problems with limited resources (CPU/memory)
the amount of time used for each PSO-cluster is limited and equal.
This means that two computationally demaning networks doesn't have time
to learn so much.

In this context GA hopefully works similarly to bayesian methods by
estimating the distribution of 'correct' NN architectures for the
given problem.


The hierarchical distribution of GA NN architectures and PSO parameters with
fixed NN architecture will be sampled according to goodness of solutions.





