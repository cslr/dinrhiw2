TODO

 * improve and fix and test parallel gradient descent to work again
   (pgrad and plbfgs)

* DBN.toNNetwork() code must use W.transpose() when transferring
  weights between DBN and nnetwork<> is this a bug??

 * test RBM optimization, set them frozen and edit
   to add extra layers, and optimize non-frozen layers
   (Kaggle)

* throughly test HMC further.
  Does negative phase improve results or not?
  - does TOP gradient results work better or worse?


LATER (multistep optimization):

* implement recurrent neural networks

  * RUN/TEST RECURRENT NNTOOL TESTS IN TOOLS DIRECTORY

* HMM (hidden markov models)

  - implement discrete HMM using arbitrary precision numbers
    * study some Hidden Markov Model theory
    * this will be used to learn and generate likely stimulation sequences
      and tag stimulation elements into language (does make sense?)
      
   
========================

- bayesian neural network fixes 
  (train both function and its inverse at the same time)

- OPTIMIZE BFGS for speed:
  * implement L-BFGS for large neural networks
    (in practice we ALWAYS have rank(H) << dim(H)
     so there is no point in trying to estimate the whole H)
  
- HMC sampler:
  [check convergence by starting N sampling threads and
   then sample until ||m_w_j - m_x_i|| converges close
   enough to zero] (mean values are close enough each other)

- GA3, ga3_test_function.h: fully implement and test
  genetic algorithm optimization for real-valued vectors.
  - change GA implementation to sort offspring and select
    the better upper part (radix sort) (+ some randomness)

- there are bugs in classes that create internal pthreads to do
  background execution. the pthread-entry functions are not
  templated to do proper pointer casts (to templated pointer type)

- bugfix and debugging:
  * valgrind, gdb

- DOCUMENTATION

----------------------------------------------------------------
OLD TODO

  - AMD64 MATH: SYLVESTER EQ SOLVER FAIL

  TEST
   - write TEST to test gramschmidt<>(vector) gives same as gramschimidt<>(matrix)
   - test dataset::convert()     
   - GDA clustering
   - retest matrix inversion code after bugfix
  test association rule finder with real data
  test datamining code


(MAYBE) BUGS/ERRORS

  avl-tree remove_node() has serious bugs:
    avl-tree infinite loops
    avl-tree forgots/drops non-removed nodes (bad)
    avl-tree isn't balanced after removal of nodes (bug).
    (calculate with paper&pencil with small examples..)

  test and/or add accuracy of symmetric eigenvalue solver
  (PCA seems to fail sometimes)

  write faster/good association rules finder (reread the relevant paper)
  - what other datamining methods would be needed/useful(?)
  - copy from own Java code

===================================================================
