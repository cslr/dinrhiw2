/*
 * nntool (narya) - 
 * a simple feedforward neural network
 * optimizer command line tool.
 * 
 * (C) copyright Tomas Ukkonen 2004, 2005
 *
 *************************************************************
 * 
 * neural networks and other machine learning 
 * models try to build generic models which can
 * describe essential features of data.
 * 
 * The goodness of compression is measured with some
 * metrics: 1. goodness of fit (MSE or some other metric),
 *          2. model complexity, 
 *          3. ability to handle, model and use
 *             non-functional (graph-like) relationships,
 *          4. ability to jointly learn many different problems
 *             and utilize common information between problems,
 *          5. ability to use invariants and be representation
 *             independent (vector dimensions etc. may change),
 *          6. ability to handle uncertainties in data, 
 *             'missing' values
 *          7. scaling to large problems and 
 *             efficient use of computational resources.
 *
 * Metrics itself must be also learnt from data.
 * This requires assumptions which can be learnt by 
 * using GAs or other competion based 
 * (survival of the fittests) algorithms.
 * 
 * 1. Unparametric methods such as GAs are used to learn good metrics: 
 *    p(metrics).
 * 2. Solution to problems are learnt with optimization: 
 *    p(params|problem, metrics)*p(metrics)
 * 
 *************************************************************
 *  
 * this will be used as a basic tool for training nns.
 * 
 * graphical interface for ready made tool should be
 * easy to build after nntool works flawlessy.
 *
 * better methods can be build for specific problems
 * if needed
 *
 * TODO: 
 * 
 * get basic backprop + PSO fNN optimizers working.
 * (from dataset). COMPILES NOW. 
 *  ====> READ CODE AND ADD REST OF THE CODE
 * 
 *
 * add import/export from/to a ascii formats described in fscanf like syntax
 * (to dataset)
 * testing
 * put to web page
 * (this is then referenced when talking to what I have done (this is naturally only
 *  small part of it). Need to have this so I can refer/talk about this project when
 *  talking to CIS people + include couple of 'papers' to CV)
 *
 * add BFGS optimization and bayesian inference methods later.
 *
 * finally add SND network engine mode 
 * (this means nntool must become separated
 *  program/project which uses both algos and dinrhiw)
 * 
 */


#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include <unistd.h>
#include <dinrhiw/dinrhiw.h>
#include <exception>

#include <vector>
#include <string>

// version history:
// 
// 0.1 initial tool for 
//      chess playing neural network training
// 0.2 the first generic tool
//
// future: 2nd order optimization methods
//         better retraining support
// 
// ++ support SND learning with interproblem
//    information sharing in daemon mode.
//    (needs some theoretical work)
  
  
#include "nntool.h"
#include "argparser.tab.h"

void print_usage(bool all);

using namespace whiteice;

int main(int argc, char** argv)
{
  try{
    std::string datafn, nnfn;
    std::string lmethod;
    std::vector<std::string> lmods;
    std::vector<unsigned int> arch;
    unsigned int cmdmode;
    unsigned int tpid;
    std::string ior, cns;
    bool daemon, verbose;
    bool overtrain = false;
    
    parse_commandline(argc, argv, datafn, nnfn, 
		      lmethod, lmods, arch, 
		      cmdmode, tpid, ior, cns,
		      daemon, verbose);
    srand(time(0));
    
    if(cmdmode != 0 || daemon == true){
      printf("Daemon and 'send command' modes aren't supported yet.\n");
      return 0;
    }
    
    // note: PCA is checked in dataset preprocessing code
    for(unsigned int i=0;i<lmods.size();i++){
      if(lmods[i] == "+ica"){
	printf("ICA isn't integrated into dataset code yet.\n");
	return 0;
      }
      else if(lmods[i] == "+ot"){
	overtrain = true; // enables overtraining
      }
    }
    
    
    // tries to open data and nnfile
    
    // loads data
    dataset<math::atlas_real<float> > data;
    bool stdinout_io = false;
    
    if(datafn.size() > 0){ // file input
      if(!data.load(datafn)){
	fprintf(stderr, "error: couldn't open datafile: %s.\n", datafn.c_str());
	exit(-1);
      }
      
      if(data.getNumberOfClusters() < 1){
	fprintf(stderr, "error: datafile is empty.\n");
	exit(-1);
      }
      
      if(lmethod != "use"){
	if(data.getNumberOfClusters() < 2){
	  fprintf(stderr, "error: datafile doesn't contain example pairs.\n");
	  exit(-1);
	}
      }
      
      
      if(arch[0] <= 0){
	arch[0] = data.dimension(0);
      }
      else if(arch[0] != data.dimension(0)){
	fprintf(stderr, "error: bad network input layer size, input data dimension pair.\n");
	exit(-1);
      }
      
      if(arch[arch.size()-1] <= 0){
	arch[arch.size()-1] = data.dimension(1);
      }
      else if(arch[arch.size()-1] != data.dimension(1)){
	fprintf(stderr, "error: bad network output layer size, output data dimension pair.\n");
	exit(-1);
      }
      
      
      if(data.size(0) == 0 || (data.size(1) == 0 && lmethod != "use")){
	fprintf(stderr, "error: empty datasets cannot be used for training.\n");
	exit(-1);
      }
      else if(lmethod != "use" && data.size(0) != data.size(1)){
	if(data.size(0) < data.size(1)){
	  printf("warning: output dataset is larger than input dataset.\n");
	  printf("some data is discarded. pairing may be incorrect.\n");
	  
	  data.resize(1, data.size(0));
	}
	else if(data.size(0) > data.size(1)){
	  printf("warning. input dataset is larger than output dataset.\n");
	  printf("some data is discarded. pairing may be incorrect.\n");
	  
	  data.resize(0, data.size(1));
	}
      }
    }
    else{
      stdinout_io = true;
      fprintf(stderr, "stdin/stdout I/O isn't supported yet.\n");    
      exit(-1);
    }
    
  
    neuralnetwork<math::atlas_real<float> >* network = 0;
    
    
    if(access(nnfn.c_str(), F_OK) == 0){ // tries to load from file
      
      if(access(nnfn.c_str(), R_OK | W_OK) == 0){
	std::cout << "arch1 size: " << arch.size() << std::endl;
	for(unsigned l=0;l<arch.size();l++)
	  std::cout << arch[l] << " ";
	std::cout << std::endl;
	
	network = new neuralnetwork<math::atlas_real<float> >(arch);
	
	if(network->load(nnfn) == false){
	  fprintf(stderr, "error: cannot load from file %s\n", nnfn.c_str());
	  delete network;
	  exit(-1);
	}
      }
      else{
	fprintf(stderr, "error: cannot access network configuration file %s\n", nnfn.c_str());
	exit(-1);
      }
    }
    else{
      std::cout << "arch2 size: " << arch.size() << std::endl;
      for(unsigned l=0;l<arch.size();l++)
	std::cout << arch[l] << " ";
      std::cout << std::endl;
      
      network = new neuralnetwork<math::atlas_real<float> >(arch);
      
      std::cout << "A: " << nnfn << std::endl;
      
      if(network->save(nnfn) == false){
	fprintf(stderr, "error: cannot write to file %s\n", nnfn.c_str());
	exit(-1);
      }
      
      std::cout << "B" << std::endl;
      
      network->randomize();
      
      std::cout << "C" << std::endl;
      
      network->setlearningrate(0.1);
      
      std::cout << "D" << std::endl;
    }
    
    
    if(verbose && !stdinout_io){
      if(lmethod == "use")
	printf("processing %d data points.\n", data.size(0));
      else
	printf("%d data points for %d -> %d mapping.\n",
	       data.size(0), data.dimension(0), data.dimension(1));
    }
    

    fflush(stdout);
    
    // preprocessing
    // (datasets may have already been preprocessed)
#if 0
    if(!stdinout_io && lmethod != "use"){
      std::vector<dataset< math::atlas_real<float> >::data_normalization> plist;
      
      // assumes there is no outliers
      plist.push_back(dataset< math::atlas_real<float> >::dnMeanVarianceNormalization);
      bool PCAadded = false;
      
      for(unsigned int i=0;i<lmods.size();i++){
	if(lmods[i] == "+pca" && PCAadded == false){
	  plist.push_back(dataset< math::atlas_real<float> >::dnCorrelationRemoval);
	  PCAadded = true;
	}
      }
      
      if(!data.convert(0, plist) || !data.convert(1, plist)){
	fprintf(stderr, "error: cannot preprocess data to wanted format.\n");
	delete network;
	exit(-1);
      }
    }
#endif
    
    
    // learning or activation
    
    if(lmethod == "bp"){
      if(verbose)
	std::cout << "Starting neural network backpropagation optimizer..." 
		  << std::endl;
      
      backpropagation< math::atlas_real<float> >* nnoptimizer = 0;
      nnoptimizer = new backpropagation<math::atlas_real<float> >();
      nnoptimizer->setData(network, &data);
      
      network->setlearningrate(0.01);
      
      if(overtrain)
	nnoptimizer->forced_improve();
      
      unsigned int iters = 0;
      std::vector< math::atlas_real<float> > gradients, history;
      history.resize(10);
      gradients.resize(history.size()-1);
      
      for(unsigned int i=0;i<history.size();i++)
	history[i] = 100000000000000000.0f;
      
      
      while(true){
	nnoptimizer->improve(10);
	
	iters += 10;
	
	// keep searching for better solution till 1st, 2nd and 
	// 3rd order derivates of currentValidationError are all 
	// positive. (ADD: or no improvemnts in N iterations)
	
	for(unsigned int i=1;i<history.size();i++)
	  history[i-1] = history[i];
	
	
	history[history.size()-1] = nnoptimizer->getCurrentError();
	
	// updates gradients
	for(unsigned int i=0;i<gradients.size();i++)
	  gradients[i] = history[history.size()-1] - history[history.size()-2-i];
	
	{
	  unsigned int count = 0;
	  for(unsigned int i=0;i<gradients.size();i++)
	    if(gradients[i] > 0) count++;
	  
	  if(count == gradients.size()){
	    //std::cout << "Heuristics: convergence detected." << std::endl;
	    //break;
	  }
	}
	
	
	if(verbose && !stdinout_io)
	  printf("MSE: %f\n", nnoptimizer->getCurrentError().value());
	
	// + add every 15mins results saving
      }
      
    }
    else if(lmethod == "grad"){
      if(verbose)
	std::cout << "Starting neural network gradient descent optimizer.."
		  << std::endl;
      
      std::cout << "ERROR: GRADIENT DESCENT IS NOT IMPLEMENTED YET."
		<< std::endl;
      
      
    }
    else if(lmethod == "nnpso"){
      if(verbose)
	std::cout << "Starting neural network particle swarm optimizer..." 
		  << std::endl;
      
      nnPSO<math::atlas_real<float> >* nnoptimizer = 0;
      nnoptimizer = new nnPSO<math::atlas_real<float> >(network, &data, 30);
      nnoptimizer->verbosity(verbose);
      
      if(overtrain)
	nnoptimizer->enableOvertraining();
      else
	nnoptimizer->disableOvertraining();
      
      unsigned int iters = 0;
      math::atlas_real<float> gradients[3], history[4];
      
      for(unsigned int i=0;i<3;i++){
	history[i] = 100000000000000000.0f;
      }
      
      
      while(true){
	nnoptimizer->improve(10);
	iters += 10;
	
	// keep searching for better solution till 1st, 2nd and 
	// 3rd order derivates of currentValidationError are all 
	// positive. (ADD: or no improvemnts in N iterations)
	
	for(unsigned int i=1;i<4;i++)
	  history[i-1] = history[i];
	
	if(!overtrain)
	  history[3] = nnoptimizer->getCurrentValidationError();
	else
	  history[3] = nnoptimizer->getCurrentError();
	
	
	
	// updates gradients
	for(unsigned int i=0;i<3;i++)
	  gradients[i] = history[3] - history[2-i];
	
	{
	  unsigned int count = 0;
	  for(unsigned int i=0;i<3;i++)
	    if(gradients[i] > 0) count++;
	  
	  if(count == 3){
	    std::cout << "Heuristics: convergence detected." << std::endl;
	    break;
	  }
	}
	
	
	
	
	if(verbose && !stdinout_io){
	  if(overtrain)
	    printf("MSE: %f\n", nnoptimizer->getCurrentError().value());
	  else
	    printf("MSE: %f\n", nnoptimizer->getCurrentValidationError().value());
	}
	
	// + add every 15mins results saving
      }
      
      
      
      
      if(!stdinout_io && verbose)
	printf("PSO optimization stopped. Final MSE %f after %d iterations.\n",
	       nnoptimizer->getError().value(), iters);
      
      
      // moves the correct solution to the network
      {
	math::vertex< math::atlas_real<float> > temp;
	bool ok = true;
	
	if(!nnoptimizer->getSolution(temp))
	  ok = false;
	
	if(ok)
	  if(!network->importdata(temp))
	    ok = false;
	
	if(ok == false){
	  delete network;
	  delete nnoptimizer;
	  fprintf(stderr, "error: importing the optimized network solution failed.\n");
	  exit(-1);
	}
	
      }
      
      delete nnoptimizer;
    }
    else if(lmethod == "bayes"){
      // finds maximum likelihood solution of p(data|params)
      // if overtraining is activated otherwise takes
      // median of from 128 samples (from p(params|data))
      // when samples are mapped to the first principal axis 
      // (PCA highest variance direction)
      
      std::cout << "Bayesian inference not implemented." << std::endl;
      
    }
    else if(lmethod == "use"){
      std::cout << "Normal use not implemented." << std::endl;
      
    }
    
    
    
    if(lmethod != "use"){
    
      // saving results: adds pre/post processings
      // to be part of neural network structure
      
      // adds dataset<> based preprocess (in) and
      // invpreprocess (out) layers
      
      std::cout << "RESULT SAVING NOT IMPLEMENTED." << std::endl;
    }

    
    
    return 0;
  }
  catch(std::exception& e){
    std::cout << "Fatal error: unexpected exception. Reason: " 
	      << e.what() << std::endl;
    return -1;
  }
  
}



void print_usage(bool all)
{
  printf("Usage: nntool [options] [data] [arch] <nnfile> [lmethod]\n");
  
  if(!all){
    printf("Try 'nntool --help' for more information.\n");
    return;
  }
  
  
  printf("Create, train and use neural network(s).\n\n");
  printf("-v             shows ETA and other details\n");
  printf("--daemon       starts nntool in daemon mode\n");
  printf("--pid PID      sends a command to a daemon mode process\n");
  printf("--ior IORFILE  sends a command to CORBA interface of a daemon process\n");
  printf("--cns dNAME    sends a command to a deamon process with a given CORBA NamingService name\n");
  printf("--help         shows this help\n");
  printf("--version      displays version and exits\n");
  printf("[data]         a source file for inputs or i/o examples (text or binary file)\n");
  printf("               (whiteice data file format or special stdin/stdout format)\n");
  printf("[arch]         the architecture of a new nn. Eg. 3-10-9, ?-10-?\n");
  printf("<nnfile>       input/output neural networks weights file\n");
  printf("[lmethod]      method: use, bp, nnpso, bayes and modifiers: +ot, +pca, +ica\n\n");
  
  printf("Report bugs to <tomas.ukkonen@iki.fi>.\n");


  
  /*

  printf("           use      no training, calculates response for given inputs\n");
  printf("           bp       backpropagation/gradient descent\n");
  printf("           nnpso    nn pso optimizer\n");
  printf("           bayes    simple bayesian inference\n");
  printf("           ot       overtrain\n");
  printf("           pca/ica  pca/ica preprocess/dimension reduction\n");
  printf("           default method is 'use'\n");
  
  OPTIONS: "--" can be used as end of options marker so that
  --daemon file can be used as data file for example

    
  EXAMPLES
  
  "echo '1 2 3' | nntool seq.nn"  may print "4.0" to stdout.

  "echo -e '1 2 3\n4\n6 7 8\n9' | nntool 3-6-1 seq.nn bp+pca+ot"
  trains 3-6-1 network with examples {((1 2 3),4),((6,7,8),9)} and uses 
  PCA to preprocess both inputs and outputs, early stopping isn't used
  and nn overtrained by trying to find the global minimum error solution.
  
  "nntool weather.data 10-4-1 wpred.nn bp+pca" uses PCA preprocessing and
  backpropagation with early stopping to train neural network to predict
  the future weather.
  
  "nntool obser.data wpred.nn" calculates predictions from observations.
  
  "nntool --daemon wpred.nn" starts nntool and tries to become daemon process.
  It implements corba inferface through which the neural network structure maybe
  used or trained. The process may be removed by killing it with appropriate signals.
  Also "nntool --daemon 3-10-7" works as expected.
  
  "-p"-switch can be used to tell nntool to send commands to nntool running in daemon mode
  "nntool -pPID". Also CORBA IOR can be used "nntool -cIORFILE" as well as
  corba component name "-dNAME" assigned by naming service. "-d" switch may be also used
  when starting nntool in daemon mode. Note that PID works only in local machine but a IOR
  and a NamingService name can be used to command nntool service running in a remote machine.

  
  TODO: make it possible also to use nntool to send data to a specific nntool 
  daemon process from a command line.
  
  It is also possible to use '?' as a number of inputs or outputs in a neural network
  architecture specification. Without PCA/ICA "?-10-?" sets inputs and outputs according
  to training data and with PCA/ICA dimension reduction for inputs is activated and
  non-interesting dimensions are removed before
  
  As an extension all files maybe also files with .ior extension containing 
  IOR to conffile or dataset CORBA inferfaces.
  
  FILE FORMATS
  
  Nntool can read two different file formats. The first one is a
  human friendly text file format and the second one is a binary file format.
  
  
  SIGNALS
  
  SIGUSR1 causes nntool to print its current state/progress/ETA.
  SIGUSR2 causes nntool to save the current neural network configuration
          and/or outputs and stop immeadiately.
  
   */
  
}
