
large neural networks are in practice memory bounded,
one runs out of memory faster than out of CPU power. This means
lots of slow swapping, so minimizing memory usage is target of
improving large scale neural network performance.

* neural network weight compression attempt (zlib)
neural network compression code doesn't work very
well with trained NN 2-100-100-2 one gets
only 8% compression rate. 
(one specific problem .. probably doesn't get any better easily)
 - zlib compression probably doesn't compress very well floating
   point data (should add right kind of bias to compress specific problem)

(at 1 GB (1/10th of the target NN size), 10% means 100 MB save ...
 at 10 GB (target NN size) one should have close to 50% compression
 in order to handle it with 4GB+cache machine ..
 this means that one necessarily needs 64bit computing because with 50%
 lossy compression NN cannot work well anymore), if 30-40% is reached
 a single 64bit machine can then handle it)



=>

must use lossy compression in order to get
considerable compression rate. 
 - implement PCA with row vectors as a set of vectors, 
   calculate E[x*x^t] and reduce dimensions so that squared error
   can be controlled. (eigenvalue decomposition). 
 - alternatively use SVM

 - how much data one can lose without (big) problems?
   (reasonably much(?) -- NN's are reasonably error tolerant)


=====================

 - nn growing should be done so that neither bias or variance
   is too big when learning. 

=====================

 - lots of theoretical work must be done before:
   knowledge sharing and reuse of non-linear inference
   of related data (optimize multiple non-linear inferences
   globally .. see theoretical notes) (try some proposed trivial but probably won't work stuff)




